---
title: EES2019 Stacking Process 
subtitle: Synthetic Variables Evaluation (Pt.2)
author: Giuseppe Carteny
date: 24.10.2021 
toc: true
output: 
  bookdown::pdf_document2:
    includes:
      in_header: First_steps_header.tex
urlcolor: RedOrange
--- 

```{r echo=FALSE, warning=FALSE, message=FALSE}


# Admin # ==============================================================================================

want = c("tidyverse", "magrittr", "haven", "data.table", "labelled", "here", "stringr", "rlang", "car",
         "caret", "DescTools", "stargazer", "kableExtra")
have = want %in% rownames(installed.packages())
if ( any(!have) ) { install.packages( want[!have] ) }
junk <- lapply(want, library, character.only = TRUE)
options(scipen = 99)

rm(list = ls())

# Source the general workflow # ========================================================================

source(here('Scripts', 'synteval_scripts', 'Synteval_gen.R'))


# Country-spec workflow # ==============================================================================

cntry = 'CY'

EES2019_cy <- EES2019 %>% filter(countryshort==cntry)
EES2019_stckd_cy <- EES2019_stckd %>% filter(countryshort==cntry)
EES2019_cdbk_cy <- EES2019_cdbk %>% filter(countryshort==cntry)

rm(cntry)

# Generic dichotomous variables estimation # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

EES2019_cy_stack <- 
  cbind(EES2019_stckd_cy,  
        lapply(data = EES2019_stckd_cy, 
               X = list('Q2', 'Q7', 'Q9_rec', 'Q25_rec'),
               stack_var = 'party',
               FUN = gendic.fun) %>% 
          do.call('cbind',.)) %>% 
  as_tibble()

# Generic distance/proximity variables estimation # - - - - - - - - - - - - - - - - - - - - - - - - - - 

EES2019_cy_stack %<>%
  cbind(.,
        lapply(data = EES2019_cy,
               cdbk = EES2019_cdbk_cy,
               stack = EES2019_cy_stack, 
               crit = 'average',
               rescale = T,
               check = F,
               keep_id = F,
               X = list('Q10','Q11','Q23'),
               FUN = gendis.fun) %>% 
          do.call('cbind',.)) %>% 
  as_tibble()


# Syntvars evaluation: Functions, variables and data frames # ==========================================

# Source auxiliary functions # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

source(here('Scripts', 'synteval_scripts', 'Synteval_auxfuns.R'))

# Country-specific data frames # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

csdf_lst <- list('std'  = EES2019_cy,
                 'cdbk' = EES2019_cdbk_cy,
                 'SDM'  = EES2019_cy_stack)


# Synthetic variables estimation variables # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

syntvars_vrbls <- list('dep'   = list('OLS'     = 'Q10_gen', 
                                      'logit'   = 'Q7_gen'),
                       'indep' = list('ctgrcl' = c('D3_rec', 'D8_rec',  'D5_rec', 'EDU_rec', 
                                                   'D1_rec', 'D7_rec'),
                                      'cntns'  =  c('D4_age', 'D10_rec')))


# Synthetic variables estimation data frames # - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


regdf_lst  <- list('OLS'   = regdf.auxfun(data        = csdf_lst$SDM,
                                          depvar      = syntvars_vrbls$dep$OLS,
                                          cat.indvar  = syntvars_vrbls$indep$ctgrcl, 
                                          cont.indvar = syntvars_vrbls$indep$cntns),
                   'logit' = regdf.auxfun(data        = csdf_lst$SDM,
                                          depvar      = syntvars_vrbls$dep$logit,
                                          cat.indvar  = syntvars_vrbls$indep$ctgrcl, 
                                          cont.indvar = syntvars_vrbls$indep$cntns))


# Relevant parties data frame # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

relprty_df <- 
  tibble('depvar'    = 
           lapply(1:length(regdf_lst$OLS), function(x){names(regdf_lst$OLS[[x]]) %>% .[2]}) %>% 
           unlist,
         'partycode' =
           lapply(1:length(regdf_lst$OLS), function(x){names(regdf_lst$OLS[[x]]) %>% .[2]}) %>% 
           unlist %>% 
           gsub('stack_','',.) %>% 
           as.numeric)  

relprty_df %<>% 
  mutate('partyname_eng' = 
           csdf_lst$cdbk %>% 
           dplyr::select(partyname_eng, Q7) %>% 
           filter(Q7 %in% relprty_df[['partycode']]) %>% 
           .[['partyname_eng']])



# Syntvars evaluation: Null and full regression models # ===============================================

set.seed(123)

fullmod_lst <- list('OLS'   = gensyn.fun(data        = csdf_lst$SDM,
                                         depvar      = syntvars_vrbls$dep$OLS,
                                         cat.indvar  = syntvars_vrbls$indep$ctgrcl, 
                                         cont.indvar = syntvars_vrbls$indep$cntns,
                                         yhat.name   = 'socdem_synt',
                                         regsum      = T),
                    'logit' = gensyn.fun(data        = csdf_lst$SDM,
                                         depvar      = syntvars_vrbls$dep$logit,
                                         cat.indvar  = syntvars_vrbls$indep$ctgrcl, 
                                         cont.indvar = syntvars_vrbls$indep$cntns,
                                         yhat.name   = 'socdem_synt',
                                         regsum      = T))

nullmod_lst <- list('OLS'   = lapply(X = regdf_lst$OLS,   regmod = 'OLS',   null_mod.auxfun),
                    'logit' = lapply(X = regdf_lst$logit, regmod = 'logit', null_mod.auxfun))


# fullmod_lst$OLS %>% lapply(.,summary)
# fullmod_lst$logit %>% lapply(.,summary)  

# Syntvars evaluation: OLS models summary # ============================================================

# stargazer::stargazer(fullmod_lst$OLS, type = 'text',
#                      column.labels = as.character(relprty_df$Q7),
#                      dep.var.labels = 'PTV',
#                      star.cutoffs = c(0.05, 0.01, 0.001),
#                      omit.stat=c("f", "ser"),
#                      header = F,
#                      style = 'ajps')

# Syntvars evaluation: logit models summary # ==========================================================

# stargazer::stargazer(fullmod_lst$logit, type = 'text',
#                      column.labels = as.character(relprty_df$Q7),
#                      dep.var.labels = 'Vote choice',
#                      star.cutoffs = c(0.05, 0.01, 0.001),
#                      omit.stat=c("f", "ser"),
#                      header = F,
#                      style = 'ajps')

# Syntvars evaluation: OLS models fit stats # ==========================================================

# RMSE and Rsq # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


ols_df <- 
  tibble(
    'depvar'  = lapply(1:length(regdf_lst$OLS), 
                       function(x){
                         names(regdf_lst$OLS[[x]]) %>% .[2]
                       }) %>% unlist,
    'model'   = rep('full',length(regdf_lst$OLS)),
    'Rsq'     = lapply(1:length(fullmod_lst$OLS),
                       function(x) {
                         fullmod_lst$OLS[[x]] %>% summary %>% .$r.squared %>% round(., 3)
                       }) %>% unlist,
    'Adj_Rsq' = lapply(1:length(fullmod_lst$OLS),
                       function(x) {
                         fullmod_lst$OLS[[x]] %>% summary %>% .$adj.r.squared %>% round(., 3)
                       }) %>% unlist,
    'AIC'     = lapply(1:length(fullmod_lst$OLS),
                       function(x) {
                         fullmod_lst$OLS[[x]] %>% AIC
                       }) %>% unlist) %>% 
  rbind(.,
        tibble(
          'depvar'  = lapply(1:length(regdf_lst$OLS), 
                             function(x){
                               names(regdf_lst$OLS[[x]]) %>% .[2]
                             }) %>% unlist,
          'model'   = rep('null',length(regdf_lst$OLS)),
          'Rsq'     = lapply(1:length(nullmod_lst$OLS),
                             function(x) {
                               nullmod_lst$OLS[[x]] %>% summary %>% .$r.squared %>% round(., 3)
                             }) %>% unlist,
          'Adj_Rsq' = lapply(1:length(nullmod_lst$OLS),
                             function(x) {
                               nullmod_lst$OLS[[x]] %>% summary %>% .$adj.r.squared %>% round(., 3)
                             }) %>% unlist,
          'AIC'     = lapply(1:length(fullmod_lst$OLS),
                             function(x) {
                               nullmod_lst$OLS[[x]] %>% AIC
                             }) %>% unlist))

ols_df %<>% 
  left_join(., relprty_df, by='depvar') %>% 
  dplyr::select(depvar, partycode, partyname_eng, model,
                Rsq, Adj_Rsq, AIC)



# Syntvars evaluation: logit models fit stats # ========================================================


fulllogit_df <- 
  tibble(
    'depvar'     = lapply(1:length(regdf_lst$logit), 
                          function(x){
                            names(regdf_lst$OLS[[x]]) %>% .[2]
                          }) %>% unlist,
    'model'      = rep('full',length(regdf_lst$logit)),
    'Ps_Rsq'     = lapply(1:length(fullmod_lst$logit),
                          function(x){
                            DescTools::PseudoR2(fullmod_lst$logit[[x]], which = 'McFadden')
                          }) %>% unlist,
    'Adj_Ps_Rsq' = lapply(1:length(fullmod_lst$logit),
                          function(x){
                            DescTools::PseudoR2(fullmod_lst$logit[[x]], which = 'McFaddenAdj')
                          }) %>% unlist,
    'AIC'        = lapply(1:length(fullmod_lst$logit),
                          function(x) {
                            fullmod_lst$logit[[x]] %>% AIC
                          }) %>% unlist
  ) %>% 
  left_join(., relprty_df, by='depvar') %>% 
  dplyr::select(depvar, partycode, partyname_eng, model,
                Ps_Rsq, Adj_Ps_Rsq, AIC)



nulllogit_df<- 
  tibble(
    'depvar'     = lapply(1:length(regdf_lst$logit), 
                          function(x){
                            names(regdf_lst$OLS[[x]]) %>% .[2]
                          }) %>% unlist,
    'model'      = rep('null',length(regdf_lst$logit)),
    'Ps_Rsq'     = lapply(1:length(nullmod_lst$logit),
                          function(x){
                            DescTools::PseudoR2(nullmod_lst$logit[[x]], which = 'McFadden')
                          }) %>% unlist,
    'Adj_Ps_Rsq' = lapply(1:length(nullmod_lst$logit),
                          function(x){
                            DescTools::PseudoR2(nullmod_lst$logit[[x]], which = 'McFaddenAdj')
                          }) %>% unlist,
    'AIC'        = lapply(1:length(fullmod_lst$logit),
                          function(x) {
                            nullmod_lst$logit[[x]] %>% AIC
                          }) %>% unlist
  ) %>% 
  left_join(., relprty_df, by='depvar') %>% 
  dplyr::select(depvar, partycode, partyname_eng, model,
                Ps_Rsq, Adj_Ps_Rsq, AIC)



```


# Introduction 

The second part of the evaluation of synthetic variables estimation consists in creating a set of 
summary documents that will part of an appendix to the SDM codebook. These documents will be created 
with `r markdown` and related packages (e.g. `bookdown`, `knitr`, `kableExtra`) using the information 
collected in our previous step. 

The creation of documents summarizing synthetic variables estimation is needed for transparency and 
reproducibility sake. Synthetic variables are the only variables of the SDM created with procedures that 
bo beyond basic transformations of existing variables. Hence, we must provide a summary of the 
statistical models to the prospective SDM users, including workarounds for solving some issues faced in 
estimating our synthetic variables. 

# Reviewing and summarizing our evaluation scripts, selecting relevant information

The first step for creating our summary documents consists in determining which information should be 
included. As you saw during the last two weeks, the existing evaluation scripts provide a lot of 
information. However, some of them are actually not needed for our documents. For instance, if partial 
models are needed just to deal with 'problematic' models, then creating partial models for *all* the 
regression models that have been estimated, and moreover adding their fit statistics to our summary data
frames, is a useless exercise. This situation was created primarily because the 
evaluation procedure was conceived when the structure of the summary was still under-development (and, 
actually, the evaluation procedure needed also some improvements). 

Thus, now that the structure of the summary documents reached an almost definitive form, the workflow 
used for evaluating synthetic variables estimation must be improved in order to become less redundant, 
clearer and more concise^[I already reviewed our 'synteval' scrit this issue, by removing from each
country-specific 'synteval' script the general workflow used for creating the SDM (namely, a partial 
version of `EES2019_stack.R`). This will return to be very useful for processing our `r markdown` files 
since they are meant to include more than one country.]. After these improvements, we will include the 
scripts in our `r markdown` documents and this will allow us to simply *remove* the evaluation scripts, 
and tidy up a bit our repositories.

To exemplify how we should tidy up our scripts I will make again  reference to the one dedicated to the 
Cypriot sample (`EES2019_cy_synteval.R` in the `~/Scripts/synt_eval_scripts/country_spec_scripts/` 
subdirectory). 

## What has to be changed

Overall, the scripts are fine until the sections dedicated to the model fit stats. Thus we run the 
country-specific workflow for creating the other generic variables, then we source the functions, create 
the variables and data frames needed for our evaluations, fit the full and null models, and finally 
summarize them with regression tables and data frames dedicated to model fit statistics.  

These steps already create more information than what we might actually need in our documents. For 
instance what is definitely needed is the data frame with information concerning the relevant parties (
the `relprty_df` object) and the `stargazer` regression tables. What we do not really need for our 
documents, are the data frames with the fit statistics about the full and null models. Nonetheless, it 
would be wise to leave them there if we decide, later on, to include them in our scripts or at least 
include some of the information included in those data frames. 

What must be changed is the **estimation and evaluation of the partial models** used for dealing with 
our full models showing inflated standar errors. During the last weeks I realised that we definitely do 
not need to create said constrained models (and, thus, a summary of them) for those full models showing 
reliable estimates. Thus we can implement said sections of our 'synteval' scripts removing some steps, 
and changing others. 

First, after the last section dedicated to full and null models fit statistics 
('*Syntvars evaluation: logit models fit stats*') I would insert a brief comment section like the 
following one:

```{r, eval=F, echo=T}

# Full models evaluation # =================

# logit models 3, and 5 show inflated SE on some predictors, more specifically: 
# Model 3: D7_rec (only for category 2)
# Model 5: D8_rec, D5_rec, EDU_rec, D7_rec (only for category 2)

# Model 3 constant term is not affected by D7_rec inflated SE, whereas Model 6 constant is affected 
# showing unusual values. In the end we deal only with model 6 affected by separation issue.

```

The we cross tab the dependent variable with the problematic predictors. I have been 